"""
å‘½ä»¤è¡Œæ¥å£ï¼Œæä¾›ç”¨æˆ·äº¤äº’å’Œç¨‹åºå…¥å£
"""
import os
import argparse
import time
import sys
import traceback
from pathlib import Path
from typing import List, Dict, Any, Optional
from termcolor import colored
from tqdm import tqdm
import concurrent.futures
from dotenv import load_dotenv
load_dotenv(".env")

# ä½¿ç”¨langchainçš„Azure OpenAIæ¥å£
from langchain_openai import AzureChatOpenAI

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import (
    BASE_DIR, FILE_DIR, TOC_DIR, KNOWLEDGE_DIR, SUMMARIES_DIR, 
    META_SUMMARY_DIR, INTEGRATED_DIR, CHUNK_SIZE, CHUNK_OVERLAP, 
    SUMMARY_INTERVAL, MAX_WORKERS, SUPPORTED_FORMATS, MODEL,
    DEPTH_OPTIONS, DEFAULT_DEPTH
)
from src.document_processor import DocumentProcessor
from src.toc_extractor import TOCExtractor
from src.chunk_analyzer import ChunkAnalyzer
from src.summary_generator import SummaryGenerator
from src.output_integrator import OutputIntegrator
from src.utils import setup_directories, format_elapsed_time


def setup_llm() -> AzureChatOpenAI:
    """åˆå§‹åŒ–AzureChatOpenAIå®ä¾‹"""
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–APIé…ç½®
    azure_api_key = os.environ.get("AZURE_API_KEY", "")
    azure_endpoint = os.environ.get("AZURE_API_BASE", "")
    azure_api_version = os.environ.get("AZURE_API_VERSION", "2025-01-01-preview")
    azure_deployment = os.environ.get("AZURE_DEPLOYMENT", "gpt-4.1")
    
    if not azure_api_key or not azure_endpoint:
        raise ValueError("ç¼ºå°‘Azure APIé…ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–.envæ–‡ä»¶")
    
    try:
        llm = AzureChatOpenAI(
            azure_deployment=azure_deployment,
            api_key=azure_api_key,
            azure_endpoint=azure_endpoint,
            api_version=azure_api_version,
        )
        return llm
    except Exception as e:
        print(colored(f"è®¾ç½®LLMæ—¶å‡ºé”™: {e}", "red"))
        traceback.print_exc()
        sys.exit(1)


def print_welcome_message():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯å’Œä½¿ç”¨è¯´æ˜"""
    welcome_text = f"""
ğŸ“š AIä¹¦ç±é˜…è¯»å·¥å…· ğŸ“š
---------------------------
åŠŸèƒ½ï¼š
1. æå–æ–‡æ¡£ç›®å½•
2. åˆ†å‰²æ–‡æœ¬ä¸ºchunksï¼ˆå¤§å°: {CHUNK_SIZE} tokensï¼Œé‡å : {CHUNK_OVERLAP} tokensï¼‰
3. åˆ†ææ¯ä¸ªchunkå¹¶æå–å…³é”®ä¿¡æ¯
4. æ¯ {SUMMARY_INTERVAL} ä¸ªchunkç”Ÿæˆä¸€ä¸ªä¸­é—´æ‘˜è¦
5. ç”Ÿæˆæ•´ä½“æ‘˜è¦
6. æ•´åˆç›®å½•ã€æ‘˜è¦å’Œæ•´ä½“æ‘˜è¦æˆå®Œæ•´æŠ¥å‘Š

æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(SUPPORTED_FORMATS)}
æ”¯æŒçš„æ·±åº¦é€‰é¡¹: {', '.join(DEPTH_OPTIONS)}

ä½¿ç”¨æ–¹å¼ï¼š
python cli.py --file your_file.md [--chunk-size 5000] [--overlap 500] [--interval 5] [--workers 3] [--depth æ ‡å‡†]

æŒ‰Enterç»§ç»­æˆ–Ctrl+Cé€€å‡º...
"""
    print(colored(welcome_text, "cyan"))
    input()


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='AIä¹¦ç±é˜…è¯»å·¥å…·')
    parser.add_argument('--file', '-f', required=True, help='è¦åˆ†æçš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--chunk-size', type=int, default=CHUNK_SIZE, help=f'åˆ†å‰²å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œé»˜è®¤ä¸º{CHUNK_SIZE}')
    parser.add_argument('--overlap', type=int, default=CHUNK_OVERLAP, help=f'chunké‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œé»˜è®¤ä¸º{CHUNK_OVERLAP}')
    parser.add_argument('--interval', type=int, default=SUMMARY_INTERVAL, help=f'æ‘˜è¦ç”Ÿæˆé—´éš”ï¼ˆchunkæ•°é‡ï¼‰ï¼Œé»˜è®¤ä¸º{SUMMARY_INTERVAL}')
    parser.add_argument('--workers', type=int, default=MAX_WORKERS, help=f'å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º{MAX_WORKERS}')
    parser.add_argument('--depth', choices=DEPTH_OPTIONS, default=DEFAULT_DEPTH, help=f'åˆ†ææ·±åº¦ (æ¦‚å¿µæ€§/æ ‡å‡†/è¯¦å°½), é»˜è®¤ä¸º{DEFAULT_DEPTH}')
    return parser.parse_args()


def process_chunks(chunks: List[str], chunk_analyzer: ChunkAnalyzer, max_workers: int) -> List[Dict[str, Any]]:
    """å¹¶è¡Œå¤„ç†chunks
    
    Args:
        chunks: åˆ†å‰²åçš„chunks
        chunk_analyzer: chunkåˆ†æå™¨
        max_workers: æœ€å¤§å¹¶è¡Œæ•°
        
    Returns:
        åˆ†æç»“æœåˆ—è¡¨
    """
    results = []
    
    print(colored(f"\nğŸ” æ­£åœ¨åˆ†æ {len(chunks)} ä¸ªchunks (å¹¶è¡Œå·¥ä½œçº¿ç¨‹: {max_workers})...", "cyan"))
    
    start_time = time.time()
    
    # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # åˆ›å»ºfutures
        futures = {executor.submit(chunk_analyzer.analyze_chunk, chunk, i): i for i, chunk in enumerate(chunks)}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="å¤„ç†chunks"):
            chunk_idx = futures[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(colored(f"\nâŒ å¤„ç†chunk {chunk_idx} æ—¶å‡ºé”™: {e}", "red"))
                results.append({
                    "chunk_idx": chunk_idx,
                    "has_content": False,
                    "knowledge": [],
                    "error": str(e)
                })
    
    elapsed_time = time.time() - start_time
    elapsed_str = format_elapsed_time(elapsed_time)
    print(colored(f"\nâœ… å®Œæˆæ‰€æœ‰chunkså¤„ç† (æ€»è€—æ—¶: {elapsed_str})", "green"))
    
    # æŒ‰ç´¢å¼•æ’åºç»“æœ
    return sorted(results, key=lambda x: x.get('chunk_idx', 0))


def generate_interval_summaries(chunks_results: List[Dict[str, Any]], summary_generator: SummaryGenerator, interval: int) -> List[str]:
    """ç”Ÿæˆé—´éš”æ‘˜è¦
    
    Args:
        chunks_results: chunkåˆ†æç»“æœ
        summary_generator: æ‘˜è¦ç”Ÿæˆå™¨
        interval: ç”Ÿæˆæ‘˜è¦çš„é—´éš”
        
    Returns:
        æ‘˜è¦åˆ—è¡¨
    """
    summaries = []
    
    # æŒ‰åŒºé—´ç”Ÿæˆæ‘˜è¦
    interval_knowledge = []
    interval_count = 0
    
    for i, result in enumerate(chunks_results):
        if result.get('has_content', False):
            # æ”¶é›†è¯¥ç‰‡æ®µçš„çŸ¥è¯†ç‚¹
            knowledge = result.get('knowledge', [])
            interval_knowledge.extend(knowledge)
        
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾é—´éš”æˆ–æœ€åä¸€ä¸ªchunk
        if (i + 1) % interval == 0 or i == len(chunks_results) - 1:
            if interval_knowledge:  # åªåœ¨æœ‰çŸ¥è¯†ç‚¹æ—¶ç”Ÿæˆæ‘˜è¦
                interval_count += 1
                summary = summary_generator.generate_interval_summary(interval_knowledge, interval_count)
                summaries.append(summary)
                interval_knowledge = []  # é‡ç½®çŸ¥è¯†ç‚¹åˆ—è¡¨
    
    return summaries


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ‰“å°æ¬¢è¿ä¿¡æ¯
        print_welcome_message()
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        file_path = Path(args.file)
        chunk_size = args.chunk_size
        chunk_overlap = args.overlap
        summary_interval = args.interval
        max_workers = args.workers
        depth = args.depth
        
        print(colored(f"\nğŸ“Š åˆ†ææ·±åº¦: {depth}", "cyan"))
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not file_path.exists():
            print(colored(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", "red"))
            return
        
        # åˆå§‹åŒ–LLM
        print(colored("ğŸ¤– åˆå§‹åŒ–Azure OpenAI...", "cyan"))
        llm = setup_llm()
        
        # è®¾ç½®ç›®å½•ç»“æ„
        dirs = setup_directories(BASE_DIR, file_path.name)
        
        # 1. å¤„ç†æ–‡æ¡£
        print(colored(f"\nğŸ“ƒ å¤„ç†æ–‡æ¡£: {file_path.name}...", "cyan"))
        doc_processor = DocumentProcessor(file_path)
        processed_file = doc_processor.process(dirs["file"])
        print(colored(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {processed_file}", "green"))
        
        # 2. åŠ è½½æ–‡æ¡£æ–‡æœ¬
        print(colored("\nğŸ“œ åŠ è½½æ–‡æ¡£æ–‡æœ¬...", "cyan"))
        document_text = doc_processor.load_text()
        print(colored(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆ: {len(document_text)} ä¸ªå­—ç¬¦", "green"))
        
        # 3. æå–ç›®å½•
        print(colored("\nğŸ“ æå–æ–‡æ¡£ç›®å½•...", "cyan"))
        toc_extractor = TOCExtractor(llm)
        toc_content = toc_extractor.extract_toc(document_text)
        toc_path = dirs["toc"] / f"{file_path.stem}_toc.md"
        toc_extractor.save_toc(toc_content, toc_path)
        
        # 4. åˆ†å‰²æ–‡æœ¬
        print(colored(f"\nâœ‚ï¸ å°†æ–‡æ¡£åˆ†å‰²æˆchunks (å¤§å°: {chunk_size}, é‡å : {chunk_overlap})...", "cyan"))
        chunks = doc_processor.split_text(document_text, chunk_size, chunk_overlap)
        print(colored(f"âœ… åˆ†å‰²å®Œæˆ: ç”Ÿæˆ {len(chunks)} ä¸ªchunks", "green"))
        
        # 5. åˆ†æchunks
        chunk_analyzer = ChunkAnalyzer(llm, dirs["knowledge"], depth=depth)
        chunk_results = process_chunks(chunks, chunk_analyzer, max_workers)
        
        # 6. ç”Ÿæˆæ‘˜è¦
        print(colored(f"\nğŸ“— ç”Ÿæˆæ‘˜è¦ (é—´éš”: {summary_interval} chunks)...", "cyan"))
        summary_generator = SummaryGenerator(llm, dirs["summaries"], dirs["meta_summary"], file_path.name, depth=depth)
        
        # 6.1 ç”Ÿæˆé—´éš”æ‘˜è¦
        interval_summaries = generate_interval_summaries(chunk_results, summary_generator, summary_interval)
        print(colored(f"âœ… ç”Ÿæˆäº† {len(interval_summaries)} ä¸ªé—´éš”æ‘˜è¦", "green"))
        
        # 6.2 ç”Ÿæˆå…ƒæ‘˜è¦
        meta_summary = summary_generator.generate_meta_summary()
        meta_summary_path = dirs["meta_summary"] / f"{file_path.stem}_meta_summary.md"
        
        # 7. æ•´åˆè¾“å‡º
        print(colored("\nğŸ”— æ•´åˆæ‰€æœ‰è¾“å‡º...", "cyan"))
        output_integrator = OutputIntegrator(llm, dirs["integrated"], file_path.name, depth=depth)
        integrated_content = output_integrator.integrate_output(toc_path, dirs["summaries"], meta_summary_path)
        
        print(colored("\nâœ¨ å¤„ç†å®Œæˆï¼âœ¨", "green", attrs=['bold']))
        integrated_path = dirs["integrated"] / f"{file_path.stem}_integrated_{depth}.md"
        print(colored(f"\næœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆè‡³: {integrated_path}", "cyan"))
        
    except KeyboardInterrupt:
        print(colored("\n\nâŒ è¿›ç¨‹è¢«ç”¨æˆ·å–æ¶ˆ", "red"))
    except Exception as e:
        print(colored(f"\n\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}", "red"))
        traceback.print_exc()


if __name__ == "__main__":
    main() 