"""
Chunkåˆ†æå™¨ï¼Œè´Ÿè´£åˆ†æå•ä¸ªchunkå¹¶æå–å…³é”®ä¿¡æ¯
"""
from pathlib import Path
from typing import Dict, Any, List
from termcolor import colored
import json
import time
from pydantic import BaseModel

from langchain_openai import AzureChatOpenAI
from config import DEFAULT_DEPTH
from prompt import get_prompt
from src.utils import save_json, format_elapsed_time


class PageContent(BaseModel):
    """çŸ¥è¯†ç‚¹æå–ç»“æœæ¨¡å‹"""
    has_content: bool
    knowledge: list[str]


class ChunkAnalyzer:
    """Chunkåˆ†æå™¨ï¼Œè´Ÿè´£åˆ†æå•ä¸ªchunkå¹¶æå–å…³é”®ä¿¡æ¯"""
    
    def __init__(self, llm: AzureChatOpenAI, output_dir: Path, depth: str = DEFAULT_DEPTH):
        """åˆå§‹åŒ–Chunkåˆ†æå™¨
        
        Args:
            llm: LangChainçš„Azure OpenAI LLMå®ä¾‹
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            depth: åˆ†ææ·±åº¦ï¼Œå¯é€‰å€¼ä¸º"conceptual"ã€"standard"ã€"detailed"
        """
        self.llm = llm
        self.output_dir = output_dir
        self.depth = depth
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def analyze_chunk(self, chunk_text: str, chunk_idx: int) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªchunkå¹¶æå–å…³é”®ä¿¡æ¯
        
        Args:
            chunk_text: chunkæ–‡æœ¬å†…å®¹
            chunk_idx: chunkç´¢å¼•
            
        Returns:
            åˆ†æç»“æœ
        """
        start_time = time.time()
        print(colored(f"\nğŸ“– å¤„ç†chunk {chunk_idx + 1}... (æ·±åº¦: {self.depth})", "yellow"))
        
        try:
            # æ ¹æ®æ·±åº¦è·å–å¯¹åº”çš„prompt
            chunk_analysis_prompt = get_prompt("chunk_analysis", self.depth)
            
            # è°ƒç”¨LLMåˆ†æchunk
            page_parser = self.llm.with_structured_output(PageContent)
            
            result = page_parser.invoke(
                [
                    {"role": "system", "content": chunk_analysis_prompt},
                    {"role": "user", "content": f"Content text: {chunk_text}"}
                ],
            )
            
            # è®¡ç®—è€—æ—¶
            elapsed_time = time.time() - start_time
            elapsed_str = format_elapsed_time(elapsed_time)
            
            if result.has_content:
                print(colored(f"âœ… æ‰¾åˆ° {len(result.knowledge)} ä¸ªçŸ¥è¯†ç‚¹ (è€—æ—¶: {elapsed_str})", "green"))
            else:
                print(colored(f"â­ï¸ è·³è¿‡å†…å®¹ (æ— ç›¸å…³ä¿¡æ¯) (è€—æ—¶: {elapsed_str})", "yellow"))
            
            # ç»„è£…è¿”å›ç»“æœ
            analysis_result = {
                "chunk_idx": chunk_idx,
                "has_content": result.has_content,
                "knowledge": result.knowledge if result.has_content else [],
                "processing_time": elapsed_time,
                "depth": self.depth
            }
            
            # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
            self._save_analysis_result(analysis_result, chunk_idx)
            
            return analysis_result
            
        except Exception as e:
            print(colored(f"âŒ åˆ†æchunkæ—¶å‡ºé”™: {e}", "red"))
            
            # è¿”å›ç©ºç»“æœ
            error_result = {
                "chunk_idx": chunk_idx,
                "has_content": False,
                "knowledge": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "depth": self.depth
            }
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            self._save_analysis_result(error_result, chunk_idx)
            
            return error_result
    
    def _save_analysis_result(self, result: Dict[str, Any], chunk_idx: int) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
        
        Args:
            result: åˆ†æç»“æœ
            chunk_idx: chunkç´¢å¼•
        """
        # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = self.output_dir / f"chunk_{chunk_idx:04d}_{self.depth}.json"
        
        # ä¿å­˜JSONæ–‡ä»¶
        save_json(result, output_file)
    
    def load_all_results(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰åˆ†æç»“æœ
        
        Returns:
            æ‰€æœ‰chunkçš„åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        # åŒ¹é…ç‰¹å®šæ·±åº¦çš„ç»“æœæ–‡ä»¶
        result_files = sorted(self.output_dir.glob(f"chunk_*_{self.depth}.json"))
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šæ·±åº¦çš„ç»“æœæ–‡ä»¶ï¼Œå°è¯•åŠ è½½æ ‡å‡†æ·±åº¦æˆ–ä»»ä½•å¯ç”¨çš„ç»“æœ
        if not result_files:
            result_files = sorted(self.output_dir.glob("chunk_*.json"))
        
        for file_path in result_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                    results.append(result)
            except Exception as e:
                print(colored(f"âš ï¸ åŠ è½½åˆ†æç»“æœæ—¶å‡ºé”™ {file_path}: {e}", "yellow"))
        
        return sorted(results, key=lambda x: x.get('chunk_idx', 0))
    
    def get_all_knowledge(self) -> List[str]:
        """è·å–æ‰€æœ‰chunkçš„çŸ¥è¯†ç‚¹åˆ—è¡¨
        
        Returns:
            æ‰€æœ‰çŸ¥è¯†ç‚¹çš„åˆ—è¡¨
        """
        results = self.load_all_results()
        all_knowledge = []
        
        for result in results:
            if result.get('has_content', False):
                all_knowledge.extend(result.get('knowledge', []))
        
        return all_knowledge 